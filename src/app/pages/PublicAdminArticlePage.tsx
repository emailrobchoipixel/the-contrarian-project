import { Header } from '../components/Header';
import { Article } from '../components/Article';
import { Footer } from '../components/Footer';

export function PublicAdminArticlePage() {
  return (
    <div className="min-h-screen bg-white">
      <Header />
      
      <Article 
        articleId="public-admin-accountability"
        title="Process Without Outcomes: Why Public Administration Protects Programs That Cannot Prove They Work"
        author="Contrarian Staff"
        credentials=""
        date="Dec 25, 2025"
        categories={['Policy']}
        imageUrl="https://images.unsplash.com/photo-1732721093883-fe195a95cc07?crop=entropy&cs=tinysrgb&fit=max&fm=jpg&ixid=M3w3Nzg4Nzd8MHwxfHNlYXJjaHwxfHxnb3Zlcm5tZW50JTIwYnVpbGRpbmclMjBwb2xpY3l8ZW58MXx8fHwxNzY2NzAxMzY1fDA&ixlib=rb-4.1.0&q=80&w=1080&utm_source=figma&utm_medium=referral"
        content={`Public administration has always been good at designing systems that process people and money, and far less good at owning the consequences when those systems fail. From early administrative thought that treated workers as standardized inputs to be measured and controlled, to later models that emphasized rules, compliance, and procedure as the definition of "good administration," the through-line is the same: government becomes excellent at producing activity (programs, offices, trainings, plans, reports) and weak at terminating activities that do not produce results. The incentive structure explains why. Bureaucracies are built to be durable, to avoid scandal, and to demonstrate fairness through uniform process; elected officials are built to claim credit for intentions and funding, and to diffuse blame when outcomes disappoint; and the administrative state, once staffed, funded, and surrounded by contractors and stakeholders, naturally develops constituencies for its own continuation. In that environment, failure is rarely treated as an input to redesign; it is treated as an argument for more resources, a bigger mandate, or a wider definition of success. This is why so many public systems persist even when the measurable outputs are weak, the outcomes are flat, and the side-effects are obvious.

DEI is a clean example of how "process accountability" replaces "outcome accountability." Many DEI programs are justified as moral imperatives and implemented as compliance architectures—trainings, statements, reporting lines, committees, dashboards—rather than as testable interventions with clear causal pathways and termination criteria. The empirical research on common corporate diversity programming is, at best, mixed, and some of the most-cited work finds that popular approaches such as mandatory training often do not produce durable change and can even trigger backlash effects, while other approaches (for example, structured managerial accountability and certain mentoring/recruiting systems) show more promise. None of that means inclusion is unimportant; it means the dominant administrative approach—mandate, ritualize, repeat—creates an evidence vacuum where spending becomes the proof of seriousness, and skepticism becomes a reputational risk. The result is predictable: programs continue even when the strongest "success" claims are about participation rates and internal sentiment measures, while downstream outcomes (hiring, retention, promotion, skill formation, productivity, service quality) remain stubbornly hard to demonstrate at scale.

Affirmative action debates reveal the deeper administrative habit: arguing policy as symbolism while ignoring second-order effects and tradeoffs. When states or systems change admissions rules, the relevant question is not only who gets admitted where, but what the policy does to degree completion, field of study, and long-run economic mobility—because those are the outcomes that determine whether policy is empowering or merely redistributing seats in selective institutions. One rigorous, large-scale analysis of California's Proposition 209 (which ended race-based affirmative action in UC admissions) found that ending affirmative action shifted underrepresented minority applicants into lower-quality institutions on average, reduced degree attainment (including in STEM), and lowered average wages by about five percent annually from ages 24–34; it also estimated a cumulative decline in the number of early-career underrepresented minority Californians earning over $100,000 by at least three percent by the mid-2010s. Those findings are frequently invoked to argue that affirmative action "worked," but they also underscore the accountability problem: the U.S. spends decades cycling between admissions rules while failing to fix the upstream production system—K–12 quality, skill formation, and institutional incentives that determine preparation. In other words, the administrative system repeatedly chooses high-salience levers with weak capacity for root-cause improvement, then treats the persistence of gaps as proof the lever must be pulled harder.

Public schooling is the largest, longest-running illustration of resource persistence without proportional outcome delivery—and the charter school fight shows how bureaucracy protects incumbency. Traditional public-school systems tend to be governed by layered accountability: budgets, staffing rules, procurement constraints, collective bargaining, compliance regimes, and political oversight. Those constraints are often defended as equity protections, yet they also make performance differences hard to act on because "fixing" schools becomes an exercise in process reform rather than results. Charter schools are not uniformly better, and the best research says effects vary by model and context; online charters, for example, can perform dramatically worse than traditional schools. But when charters are high-performing and oversubscribed, lottery-based studies and large multi-state analyses repeatedly find meaningful gains—especially in urban settings and for disadvantaged students. Stanford CREDO's National Charter School Study III (2014–2019) reports an average advantage for charter students of about 16 additional "days of learning" in reading and 6 in math compared with matched traditional public school peers, with stronger growth for students in poverty and for Black and Hispanic students in charters. In Boston, quasi-experimental lottery evidence shows sizable achievement impacts and longer-run shifts toward four-year college pathways for charter attendees. New York City lottery-based analyses similarly find positive achievement effects for many charters over the period studied. The administrative question is therefore straightforward: if some schools consistently produce better outcomes for the same kinds of kids at comparable per-pupil spending, why would government block replication? Yet in many places, political and bureaucratic incentives favor protecting the existing delivery system over scaling demonstrably effective alternatives—because the system is structured to preserve institutions, jobs, and negotiated rules, not to maximize measured learning growth.

Minority success despite the system matters because it exposes a core attribution failure in public administration: widely cited government programs claim credit for improvements they neither designed nor causally produced. Increases in income, reductions in poverty, and growth in minority-owned businesses are routinely attributed to DEI initiatives, affirmative-action legacies, or equity-branded spending, even though those programs lack mechanisms that plausibly generate those outcomes at scale. DEI programs, for example, primarily operate through training, reporting requirements, and internal governance structures; they do not increase labor demand, expand skills in scarce occupations, raise productivity, reduce crime, lower housing costs, or improve K–12 instructional quality—the channels through which income, poverty, and entrepreneurship actually change. Similarly, affirmative-action frameworks affect admissions at the margin for a small subset of institutions but do not explain broad population-level gains occurring across regions, industries, and cohorts. When Black poverty falls to a historic low, or when Black-owned firms nearly double in number and revenue over five years, those outcomes align far more closely with tight labor markets, post-pandemic wage compression at the bottom, expanded remote-work geography, sectoral growth, access to capital, and entrepreneurial self-selection than with compliance-oriented equity programs whose outputs are trainings delivered or policies adopted. The inability of these programs to specify a causal pathway, define a counterfactual, or isolate their marginal contribution means they cannot legitimately claim ownership of the results. From a public-administration perspective, this is not a moral critique but a design failure: programs persist because they signal values and absorb funding, not because they meet the evidentiary standards required to demonstrate that they materially changed outcomes that would not have occurred otherwise.

A serious public-administration rewrite of today's equity debates would therefore stop arguing about intentions and start arguing about institutional design: how government defines success, how it measures it, how it learns, and how it shuts things down. If agencies want to fund DEI, affirmative action-adjacent initiatives, or K–12 reforms, they should be required to publish (1) a falsifiable theory of change, (2) a small number of outcome metrics tied to service delivery and economic mobility, (3) comparison baselines (similar populations without the intervention), and (4) a termination rule—what evidence would trigger redesign or defunding. That approach aligns with what the best administrative theorists eventually conceded: people are not interchangeable "inputs," and systems can't be managed by process alone; but if you refuse to measure outcomes, you are not protecting equity—you are protecting the administrative machine.`}
      />

      <Footer />
    </div>
  );
}